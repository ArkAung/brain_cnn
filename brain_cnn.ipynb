{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_DIM = 64\n",
    "STRIP_WIDTH = 512\n",
    "STRIP_HEIGHT = 512\n",
    "SLICE = 1\n",
    "SNAPS = 8 ##Number of snaps over 4 seconds\n",
    "CHANNELS = 1\n",
    "CLASSES = 2\n",
    "# CHANNELS = 8\n",
    "conv1_filter = 4\n",
    "conv2_filter = 4\n",
    "conv3_filter = 8\n",
    "conv4_filter = 8\n",
    "conv5_filter = 16\n",
    "conv6_filter = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "experiment = '8snap'\n",
    "path = './datasets/' + experiment + '/'\n",
    "\n",
    "_04847_img = np.load(path + '4847_' + experiment + '-image.npy')\n",
    "_04799_img = np.load(path + '4799_' + experiment + '-image.npy')\n",
    "_04820_img = np.load(path + '4820_' + experiment + '-image.npy')\n",
    "_05675_img = np.load(path + '5675_' + experiment + '-image.npy')\n",
    "_05680_img = np.load(path + '5680_' + experiment + '-image.npy')\n",
    "_05710_img = np.load(path + '5710_' + experiment + '-image.npy')\n",
    "\n",
    "_04847_lbl = np.load(path + '4847_' + experiment + '-label-onehot.npy')\n",
    "_04799_lbl = np.load(path + '4799_' + experiment + '-label-onehot.npy')\n",
    "_04820_lbl = np.load(path + '4820_' + experiment + '-label-onehot.npy')\n",
    "_05675_lbl = np.load(path + '5675_' + experiment + '-label-onehot.npy')\n",
    "_05680_lbl = np.load(path + '5680_' + experiment + '-label-onehot.npy')\n",
    "_05710_lbl = np.load(path + '5710_' + experiment + '-label-onehot.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_img = np.vstack((_04847_img[5:,], _04799_img[5:,], _04820_img[5:,], _05675_img[5:,], _05680_img[5:,]))\n",
    "train_lbl = np.vstack((_04847_lbl[5:,], _04799_lbl[5:,], _04820_lbl[5:,], _05675_lbl[5:,], _05680_lbl[5:,]))\n",
    "val_img = np.vstack((_04847_img[:5,], _04799_img[:5,], _04820_img[:5,], _05675_img[:5,], _05680_img[:5,]))\n",
    "val_lbl = np.vstack((_04847_lbl[:5,], _04799_lbl[:5,], _04820_lbl[:5,], _05675_lbl[:5,], _05680_lbl[:5,]))\n",
    "#val_img = _05710_img\n",
    "#val_lbl = _05710_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_img (375, 512, 512)\n",
      "train_lbl (375, 2)\n",
      "val_img (25, 512, 512)\n",
      "val_lbl (25, 2)\n"
     ]
    }
   ],
   "source": [
    "print 'train_img', train_img.shape\n",
    "print 'train_lbl', train_lbl.shape\n",
    "print 'val_img', val_img.shape\n",
    "print 'val_lbl', val_lbl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, STRIP_HEIGHT, STRIP_WIDTH], name='x')\n",
    "x_image = tf.reshape(x, [-1, STRIP_HEIGHT, STRIP_WIDTH, CHANNELS], name='x_reshaped')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, CLASSES], name='labels')\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob_fc1')\n",
    "\n",
    "\n",
    "# 3 convolutional layers\n",
    "conv1 = conv_layer(x_image, CHANNELS, conv1_filter, phase_train, 'conv1', 8, 8)\n",
    "conv2 = conv_layer(conv1, conv1_filter, conv2_filter, phase_train, 'conv2', 5, 5)\n",
    "conv3 = conv_layer(conv2, conv2_filter, conv3_filter, phase_train, 'conv3', 3, 3)\n",
    "\n",
    "# maxpool\n",
    "max_pool_1 = max_pool_layer(conv3, name='max_pool_1')\n",
    "\n",
    "# 2 convolutional layers\n",
    "conv4 = conv_layer(max_pool_1, conv3_filter, conv4_filter, phase_train, 'conv4', 3, 3)\n",
    "conv5 = conv_layer(conv4, conv4_filter, conv5_filter, phase_train, 'conv5', 4, 4)\n",
    "\n",
    "# maxpool\n",
    "max_pool_2 = max_pool_layer(conv5, name='max_pool_2')\n",
    "\n",
    "conv6 = conv_layer(max_pool_2, conv5_filter, conv6_filter, phase_train, 'conv6', 4, 4)\n",
    "max_pool_3 = max_pool_layer(conv6, name='max_pool_3')\n",
    "\n",
    "max_pool_3_flat = tf.reshape(max_pool_3, [-1, 64 * 64 * conv6_filter]) # flatten \n",
    "\n",
    "fc1 = fc_layer(max_pool_3_flat, 64 * 64 * conv6_filter, 1024, phase_train, 'fc1') # fc_layer fc1\n",
    "fc1_drop = tf.nn.dropout(fc1, keep_prob) # dropout on fc1\n",
    "fc2 = fc_layer(tf.nn.relu(fc1_drop), 1024, 512, phase_train, name='fc2') # fc_layer fc2\n",
    "fc3 = fc_layer(tf.nn.relu(fc2), 512, 256, phase_train, name='fc3') # fc_layer fc3\n",
    "fc4 = fc_layer(tf.nn.relu(fc3), 256, 128, phase_train, name='fc4') # fc_layer fc4\n",
    "fc5 = fc_layer(tf.nn.relu(fc4), 128, 64, phase_train, name='fc5') # fc_layer fc5\n",
    "y_conv = fc_layer(fc5, 64, 2, phase_train, 'y_conv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_epochs = 200\n",
    "starter_learning_rate = 1e-5\n",
    "step = 400\n",
    "decay = 0.5\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                step, decay, staircase=True)\n",
    "\n",
    "step, train_acc_arr, train_loss_arr, val_acc_arr, val_loss_arr = 0, [], [], [], []\n",
    "\n",
    "with tf.name_scope('xent'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    cross_entropy_count = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy,\n",
    "                                                                global_step = global_step)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy_count = tf.reduce_sum(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "# tf.summary.image('input', x_image, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate: 0.00001\n",
      "step 100, train accuracy 0.5360, train loss 0.76645, val accuracy 0.52, val loss 0.897393\n",
      "Current learning rate: 0.00001\n",
      "step 200, train accuracy 0.7707, train loss 0.56728, val accuracy 0.72, val loss 0.632643\n",
      "Current learning rate: 0.00001\n",
      "step 300, train accuracy 0.6133, train loss 0.61279, val accuracy 0.56, val loss 0.615041\n",
      "Current learning rate: 0.00000\n",
      "step 400, train accuracy 0.6027, train loss 0.69427, val accuracy 0.56, val loss 0.691548\n",
      "Current learning rate: 0.00000\n",
      "step 500, train accuracy 0.8773, train loss 0.42213, val accuracy 0.68, val loss 0.687557\n",
      "Current learning rate: 0.00000\n",
      "step 600, train accuracy 0.7893, train loss 0.44088, val accuracy 0.68, val loss 0.61111\n",
      "Current learning rate: 0.00000\n",
      "step 700, train accuracy 0.9253, train loss 0.33367, val accuracy 0.72, val loss 0.594096\n",
      "Current learning rate: 0.00000\n",
      "step 800, train accuracy 0.7760, train loss 0.40327, val accuracy 0.52, val loss 0.64126\n",
      "Current learning rate: 0.00000\n",
      "step 900, train accuracy 0.9813, train loss 0.23041, val accuracy 0.72, val loss 0.655405\n",
      "Current learning rate: 0.00000\n",
      "step 1000, train accuracy 0.9840, train loss 0.22112, val accuracy 0.64, val loss 0.627295\n",
      "Current learning rate: 0.00000\n",
      "step 1100, train accuracy 0.9787, train loss 0.20237, val accuracy 0.72, val loss 0.577159\n",
      "Current learning rate: 0.00000\n",
      "step 1200, train accuracy 0.9973, train loss 0.17710, val accuracy 0.64, val loss 0.676092\n",
      "Current learning rate: 0.00000\n",
      "step 1300, train accuracy 1.0000, train loss 0.14501, val accuracy 0.72, val loss 0.634078\n",
      "Current learning rate: 0.00000\n",
      "step 1400, train accuracy 1.0000, train loss 0.14817, val accuracy 0.72, val loss 0.602123\n",
      "Current learning rate: 0.00000\n",
      "step 1500, train accuracy 0.9973, train loss 0.13237, val accuracy 0.64, val loss 0.666476\n",
      "Current learning rate: 0.00000\n",
      "step 1600, train accuracy 1.0000, train loss 0.11901, val accuracy 0.72, val loss 0.589281\n",
      "Current learning rate: 0.00000\n",
      "step 1700, train accuracy 1.0000, train loss 0.12408, val accuracy 0.76, val loss 0.588278\n",
      "Current learning rate: 0.00000\n",
      "step 1800, train accuracy 1.0000, train loss 0.11045, val accuracy 0.76, val loss 0.607429\n",
      "Current learning rate: 0.00000\n",
      "step 1900, train accuracy 1.0000, train loss 0.10906, val accuracy 0.76, val loss 0.624893\n",
      "Current learning rate: 0.00000\n",
      "step 2000, train accuracy 1.0000, train loss 0.11096, val accuracy 0.76, val loss 0.587025\n",
      "Current learning rate: 0.00000\n",
      "step 2100, train accuracy 1.0000, train loss 0.11060, val accuracy 0.76, val loss 0.59046\n",
      "Current learning rate: 0.00000\n",
      "step 2200, train accuracy 1.0000, train loss 0.10224, val accuracy 0.76, val loss 0.602152\n",
      "Current learning rate: 0.00000\n",
      "step 2300, train accuracy 1.0000, train loss 0.10718, val accuracy 0.68, val loss 0.65119\n",
      "Current learning rate: 0.00000\n",
      "step 2400, train accuracy 1.0000, train loss 0.10684, val accuracy 0.76, val loss 0.611345\n",
      "Current learning rate: 0.00000\n",
      "step 2500, train accuracy 1.0000, train loss 0.10134, val accuracy 0.76, val loss 0.621137\n",
      "Current learning rate: 0.00000\n",
      "step 2600, train accuracy 1.0000, train loss 0.10150, val accuracy 0.76, val loss 0.624468\n",
      "Current learning rate: 0.00000\n",
      "step 2700, train accuracy 1.0000, train loss 0.09850, val accuracy 0.76, val loss 0.619419\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('/tmp/brain/4')\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    for batch_xs, batch_ys in next_batch(train_img, train_lbl, 5):\n",
    "        feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 1.0, phase_train: True}\n",
    "#         if step % 5 == 0:\n",
    "#             s = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "#             writer.add_summary(s, step)\n",
    "\n",
    "        train_step.run(feed_dict=feed_dict)\n",
    "        step += 1\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            \n",
    "            train_loss, train_accuracy = loss_accuracy(cross_entropy_count, accuracy_count, x, y_, keep_prob, phase_train, train_img, train_lbl, 5)\n",
    "            val_loss, val_accuracy = loss_accuracy(cross_entropy_count, accuracy_count, x, y_, keep_prob, phase_train, val_img, val_lbl, 5)\n",
    "            \n",
    "            train_acc_arr.append(train_accuracy), train_loss_arr.append(train_loss)\n",
    "            val_acc_arr.append(val_accuracy), val_loss_arr.append(val_loss)\n",
    "\n",
    "            print learning_rate.eval()\n",
    "            print \"step %d, train accuracy %.4f, train loss %.5f, val accuracy %g, val loss %g\" % (\n",
    "                step, train_accuracy, train_loss, val_accuracy, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = loss_accuracy(cross_entropy_count, accuracy_count, x, y_, keep_prob, phase_train,\n",
    "                                         val_img, val_lbl, 100)\n",
    "print 'Val Accuracy: %g, Val Loss: %g' % (test_accuracy * 100, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "get_available_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val_img[1,:,:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    plt.show(plt.imshow(val_img[1,:,:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.show(plt.imshow(np.concatenate(val_img[1], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.show(plt.imshow(np.concatenate((val_img[1,:,:,0], val_img[1,:,:,1], val_img[1,:,:,2], val_img[1,:,:,3], val_img[1,:,:,4], val_img[1,:,:,5], val_img[1,:,:,6], val_img[1,:,:,7]), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.show(plt.imshow(np.concatenate(val_img[1,:,:,[0,1,2,3,4,5,6,7]], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "val_img[1].T.T.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
